{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, to_date, col, trim, ltrim, rtrim, when, regexp_replace, monotonically_increasing_id\n",
    "\n",
    "import os\n",
    "import configparser\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import time\n",
    "\n",
    "from create_tables import create_tables, drop_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Postgres parameters\n",
    "\n",
    "db_properties={}\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"parameters.cfg\")\n",
    "db_prop = config['postgresql_jdbc']\n",
    "db_url = db_prop['url']\n",
    "db_user = db_prop['username']\n",
    "db_password = db_prop['password']\n",
    "db_properties['username']=db_prop['username']\n",
    "db_properties['password']=db_prop['password']\n",
    "db_properties['url']=db_prop['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def create_spark_session():\n",
    "\n",
    "\"\"\"\n",
    "Description: This function  creates or get (if already exists) a Spark session \n",
    "\n",
    "Arguments:\n",
    "    None\n",
    "\n",
    "Returns:\n",
    "    spark: Spark session\n",
    "\"\"\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()\n",
    "#return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Dropping tables...\n",
      "--- It took 0.05234885215759277 seconds ---\n",
      "2. Creating tables...\n",
      "--- It took 0.07380223274230957 seconds ---\n"
     ]
    }
   ],
   "source": [
    "conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(*config['postgresql_sql'].values()))\n",
    "cur = conn.cursor()\n",
    "\n",
    "print(\"1. Dropping tables...\")\n",
    "start_time = time.time()\n",
    "drop_tables(cur, conn)\n",
    "print(\"--- It took %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "print(\"2. Creating tables...\")\n",
    "start_time = time.time()\n",
    "create_tables(cur, conn)\n",
    "print(\"--- It took %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading covid data file\n",
      "Writing covid data to Postgres\n"
     ]
    }
   ],
   "source": [
    "# get filepath to covid data file\n",
    "input_data = \"data/csse_covid_19_data/csse_covid_19_daily_reports/\"\n",
    "covid_data = os.path.join(input_data, \"*.csv\")\n",
    "\n",
    "# read covid data and write it in postgres\n",
    "print (\"Reading covid data file\") \n",
    "df = spark.read.csv(covid_data, header=True, inferSchema=True)\n",
    "\n",
    "print(\"Writing covid data to Postgres\")\n",
    "\n",
    "df.select(\"*\") \\\n",
    "   .withColumn(\"fact_covid_id\", monotonically_increasing_id()) \\\n",
    "   .withColumn(\"country\", col(\"country_region\")) \\\n",
    "   .withColumn(\"Longtitude\", col(\"Long_\").cast(\"float\")) \\\n",
    "   .withColumn(\"Latitude\", col(\"Lat\").cast(\"float\")) \\\n",
    "   .withColumn(\"Incident_Rate\", col(\"Incident_Rate\").cast(\"float\")) \\\n",
    "   .withColumn(\"Case_Fatality_Ratio\", col(\"Case_Fatality_Ratio\").cast(\"float\")) \\\n",
    "   .withColumn(\"Confirmed\", col(\"Confirmed\").cast(\"integer\")) \\\n",
    "   .withColumn(\"Deaths\", col(\"Deaths\").cast(\"integer\")) \\\n",
    "   .withColumn(\"Recovered\", col(\"Recovered\").cast(\"integer\")) \\\n",
    "   .withColumn(\"Active\", col(\"Active\").cast(\"integer\")) \\\n",
    "   .withColumn(\"Last_Update\", col(\"Last_Update\").cast(\"date\")) \\\n",
    "   .drop(\"FIPS\", \"Admin2\", \"Long_\", \"Lat\", \"country_region\") \\\n",
    "   .write \\\n",
    "   .mode(\"append\") \\\n",
    "   .format(\"jdbc\") \\\n",
    "   .option(\"url\", db_url) \\\n",
    "   .option(\"dbtable\", \"fact_covid\") \\\n",
    "   .option(\"user\", db_user) \\\n",
    "   .option(\"password\", db_password) \\\n",
    "   .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating countries table\n"
     ]
    }
   ],
   "source": [
    "# Import countries data\n",
    "\n",
    "print(\"Creating countries table\")\n",
    "\n",
    "df_countries = spark \\\n",
    "    .read \\\n",
    "    .format(\"csv\", ) \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .load(\"data/countries.csv\")\n",
    "\n",
    "df_countries.select(\"*\") \\\n",
    "    .withColumn(\"Population\", col(\"Population\").cast(\"integer\")) \\\n",
    "    .withColumn(\"Area\", col(\"Area\").cast(\"integer\")) \\\n",
    "    .withColumn(\"Country\", rtrim(col(\"Country\"))) \\\n",
    "    .withColumn(\"Region\", rtrim(col(\"Region\"))) \\\n",
    "    .withColumn(\"InfantMortality\", col(\"Infant mortality\").cast(\"float\")) \\\n",
    "    .withColumn(\"NetMigration\", col(\"Net migration\")) \\\n",
    "    .withColumn(\"Coastline\", col(\"Coastline\").cast(\"float\")) \\\n",
    "    .withColumn(\"NetMigration\", col(\"Net migration\").cast(\"float\")) \\\n",
    "    .withColumn(\"GDP\", col(\"GDP\").cast(\"float\")) \\\n",
    "    .withColumn(\"Literacy\", col(\"Literacy\").cast(\"long\")) \\\n",
    "    .withColumn(\"Phones\", col(\"Phones\").cast(\"float\")) \\\n",
    "    .withColumn(\"Arable\", col(\"Arable\").cast(\"float\")) \\\n",
    "    .withColumn(\"Crops\", col(\"Crops\").cast(\"float\")) \\\n",
    "    .withColumn(\"Other\", col(\"Other\").cast(\"float\")) \\\n",
    "    .drop(\"Net migration\", \"Infant mortality\") \\\n",
    "    .write \\\n",
    "    .mode(\"append\") \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", db_url) \\\n",
    "    .option(\"dbtable\", \"dim_countries\") \\\n",
    "    .option(\"user\", db_user) \\\n",
    "    .option(\"password\", db_password) \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating exposure table\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pgrag\\AppData\\Local\\Temp/ipykernel_2316/3365766638.py:9: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  pd_df_exposure.columns = pd_df_exposure.columns.str.replace(\"[ ]\", \"_\")\n",
      "C:\\Users\\pgrag\\AppData\\Local\\Temp/ipykernel_2316/3365766638.py:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  pd_df_exposure.columns = pd_df_exposure.columns.str.replace(\"[,]\", \"\")\n"
     ]
    }
   ],
   "source": [
    "# Import Economic exposure data\n",
    "\n",
    "print(\"Creating exposure table\")\n",
    "\n",
    "pd_df_exposure = pd.read_csv(\"data/exposure.csv\", sep=\";\", encoding=\"ISO-8859-1\", dtype=str)\n",
    "pd_df_exposure = pd_df_exposure.replace({'x': None, 'No data': None, '0': None})\n",
    "pd_df_exposure = pd_df_exposure.apply(lambda x: x.str.replace(',','.'))\n",
    "pd_df_exposure.columns = pd_df_exposure.columns.str.strip()\n",
    "pd_df_exposure.columns = pd_df_exposure.columns.str.replace(\"[ ]\", \"_\")\n",
    "pd_df_exposure.columns = pd_df_exposure.columns.str.replace(\"[,]\", \"\")\n",
    "\n",
    "df_exposure = spark.createDataFrame(pd_df_exposure.astype(str))\n",
    "\n",
    "df_exposure.select(\"*\") \\\n",
    "    .withColumn(\"Net_ODA_received_perc_of_GNI\", col(\"Net_ODA_received_perc_of_GNI\").cast(\"float\")) \\\n",
    "    .withColumn(\"Aid_dependence\", col(\"Aid_dependence\").cast(\"float\")) \\\n",
    "    .withColumn(\"Volume_of_remittances\", col(\"Volume_of_remittances_in_USD_as_a_proportion_of_total_GDP_percent_2014-18\").cast(\"float\")) \\\n",
    "    .withColumn(\"Remittances\", col(\"Remittances\").cast(\"float\")) \\\n",
    "    .withColumn(\"Food_imports_percent_of_total_merchandise_exports\", col(\"Food_imports_percent_of_total_merchandise_exports\").cast(\"float\")) \\\n",
    "    .withColumn(\"food_import_dependence\", col(\"food_import_dependence\").cast(\"float\")) \\\n",
    "    .withColumn(\"Fuels_ores_and_metals_exports\", col(\"Fuels_ores_and_metals_exports\").cast(\"float\")) \\\n",
    "    .withColumn(\"primary_commodity_export_dependence\", col(\"primary_commodity_export_dependence\").cast(\"float\")) \\\n",
    "    .withColumn(\"tourism_as_percentage_of_GDP\", col(\"tourism_as_percentage_of_GDP\").cast(\"float\")) \\\n",
    "    .withColumn(\"tourism_dependence\", col(\"tourism_dependence\").cast(\"float\")) \\\n",
    "    .withColumn(\"General_government_gross_debt_Percent_of_GDP_2019\", col(\"General_government_gross_debt_Percent_of_GDP_2019\").cast(\"float\")) \\\n",
    "    .withColumn(\"Government_indeptedness\", col(\"Government_indeptedness\").cast(\"float\")) \\\n",
    "    .withColumn(\"Total_reserves_in_months_of_imports_2018\", col(\"Total_reserves_in_months_of_imports_2018\").cast(\"float\")) \\\n",
    "    .withColumn(\"Foreign_currency_reserves\", col(\"Foreign_currency_reserves\").cast(\"float\")) \\\n",
    "    .withColumn(\"Foreign_direct_investment_net_inflows_percent_of_GDP\", col(\"Foreign_direct_investment_net_inflows_percent_of_GDP\").cast(\"float\")) \\\n",
    "    .withColumn(\"Foreign_direct_investment\", col(\"Foreign_direct_investment\").cast(\"float\")) \\\n",
    "    .withColumn(\"Covid_19_Economic_exposure_index\", col(\"Covid_19_Economic_exposure_index\").cast(\"float\")) \\\n",
    "    .withColumn(\"Covid_19_Economic_exposure_index\", col(\"Covid_19_Economic_exposure_index\").cast(\"float\")) \\\n",
    "    .withColumn(\"Covid_19_Economic_exposure_index_Ex_aid_and_FDI\", col(\"Covid_19_Economic_exposure_index_Ex_aid_and_FDI\").cast(\"float\")) \\\n",
    "    .withColumn(\"Covid_19_Economic_exposure_index_Ex_aid_and_FDI_and_food_import\", col(\"Covid_19_Economic_exposure_index_Ex_aid_and_FDI_and_food_import\").cast(\"float\")) \\\n",
    "    .drop(\"Volume_of_remittances_in_USD_as_a_proportion_of_total_GDP_percent_2014-18\") \\\n",
    "    .write \\\n",
    "    .mode(\"append\") \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", db_url) \\\n",
    "    .option(\"dbtable\", \"dim_exposure\") \\\n",
    "    .option(\"user\", db_user) \\\n",
    "    .option(\"password\", db_password) \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vaccination table\n"
     ]
    }
   ],
   "source": [
    "# Import Vaccination data\n",
    "\n",
    "print(\"Creating vaccination table\")\n",
    "\n",
    "df_vaccination = spark \\\n",
    "    .read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .load(\"data/vaccination.csv\")\n",
    "\n",
    "df_vaccination.select(\"*\") \\\n",
    "    .withColumn(\"country\", col(\"location\")) \\\n",
    "    .withColumn(\"date\", regexp_replace(\"date\", \"/\", \"\").cast(\"date\")) \\\n",
    "    .withColumn(\"total_vaccinations\", col(\"total_vaccinations\").cast(\"integer\")) \\\n",
    "    .withColumn(\"people_vaccinated\", col(\"people_vaccinated\").cast(\"integer\")) \\\n",
    "    .withColumn(\"people_fully_vaccinated\", col(\"people_fully_vaccinated\").cast(\"integer\")) \\\n",
    "    .withColumn(\"total_boosters\", col(\"total_boosters\").cast(\"integer\")) \\\n",
    "    .withColumn(\"daily_vaccinations_raw\", col(\"daily_vaccinations_raw\").cast(\"integer\")) \\\n",
    "    .withColumn(\"daily_vaccinations\", col(\"daily_vaccinations\").cast(\"integer\")) \\\n",
    "    .withColumn(\"total_vaccinations_per_hundred\", col(\"total_vaccinations_per_hundred\").cast(\"float\")) \\\n",
    "    .withColumn(\"people_vaccinated_per_hundred\", col(\"people_vaccinated_per_hundred\").cast(\"float\")) \\\n",
    "    .withColumn(\"people_fully_vaccinated_per_hundred\", col(\"people_fully_vaccinated_per_hundred\").cast(\"float\")) \\\n",
    "    .withColumn(\"total_boosters_per_hundred\", col(\"total_boosters_per_hundred\").cast(\"float\")) \\\n",
    "    .withColumn(\"daily_vaccinations_per_million\", col(\"daily_vaccinations_per_million\").cast(\"integer\")) \\\n",
    "    .drop(\"location\") \\\n",
    "    .write \\\n",
    "    .mode(\"append\") \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", db_url) \\\n",
    "    .option(\"dbtable\", \"dim_vaccination\") \\\n",
    "    .option(\"user\", db_user) \\\n",
    "    .option(\"password\", db_password) \\\n",
    "    .save()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e254ffc773357790aa04a01fb60c7c6721ec5a0c6f1763bcb2e925b3d380624c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
