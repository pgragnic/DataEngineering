{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import configparser\r\n",
    "from datetime import datetime\r\n",
    "import os\r\n",
    "import pandas as pd\r\n",
    "import calendar\r\n",
    "import time\r\n",
    "from pyspark.sql import SparkSession\r\n",
    "from pyspark.sql.functions import udf, col, year, month, dayofmonth, hour, weekofyear, date_format, monotonically_increasing_id"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "config = configparser.ConfigParser()\r\n",
    "config.read('dl.cfg')\r\n",
    "\r\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]= config['default']['AWS_ACCESS_KEY_ID']\r\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]= config['default']['AWS_SECRET_ACCESS_KEY']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def create_spark_session():\r\n",
    "    spark = SparkSession \\\r\n",
    "        .builder \\\r\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.4\") \\\r\n",
    "        .getOrCreate()\r\n",
    "    return spark"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def process_song_data(spark, input_data, output_data):\r\n",
    "   # get filepath to song data file\r\n",
    "   song_data = os.path.join(input_data, \"song_data/A/*/*/*.json\")\r\n",
    "\r\n",
    "   # read song data file\r\n",
    "   print (\"    Reading song data file\") \r\n",
    "   df = spark.read.json(song_data)\r\n",
    "\r\n",
    "   # write songs table to parquet files partitioned by year and artist\r\n",
    "   print (\"    Writing songs table to parquet files\")\r\n",
    "   df.select(\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\") \\\r\n",
    "      .write.mode(\"overwrite\") \\\r\n",
    "      .partitionBy(\"year\",\"artist_id\") \\\r\n",
    "      .parquet(output_data + \"songs\")\r\n",
    "\r\n",
    "   print (\"    Writing artists table to parquet files\")\r\n",
    "   # write artists table to parquet files\r\n",
    "   df.select(\"artist_id\", \"artist_name\", \"artist_location\", \"artist_latitude\", \"artist_longitude\") \\\r\n",
    "      .write.mode(\"overwrite\") \\\r\n",
    "      .parquet(output_data + \"artists\")\r\n",
    "      \r\n",
    "   df.createOrReplaceTempView(\"songsData\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def process_log_data(spark, input_data, output_data):\r\n",
    "    \r\n",
    "    # get filepath to log data file\r\n",
    "    log_data = os.path.join(\"data/log_data/*/*/*.json\")\r\n",
    "\r\n",
    "    # read log data file\r\n",
    "    print (\"    Reading log data file\") \r\n",
    "    df = spark.read.json(log_data)\r\n",
    "\r\n",
    "    # filter by actions for song plays\r\n",
    "    dfNextSong = df.filter(df.page == \"NextSong\")\r\n",
    "    \r\n",
    "    # write users table to parquet files\r\n",
    "    print (\"    Writing users table to parquet files\")\r\n",
    "\r\n",
    "    df.select(df.userId.alias(\"user_id\"), df.firstName.alias(\"first_name\") , \\\r\n",
    "        df.lastName.alias(\"last_name\"), \"gender\", \"level\") \\\r\n",
    "        .distinct() \\\r\n",
    "        .write.mode(\"overwrite\") \\\r\n",
    "        .parquet(output_data + \"users\")\r\n",
    "\r\n",
    "    # create timestamp column from original timestamp column\r\n",
    "    get_timestamp = udf(lambda x: str(int(int(x)/1000)))\r\n",
    "    df = df.withColumn('timestamp', get_timestamp(df.ts))\r\n",
    "\r\n",
    "    # create datetime column from original timestamp column\r\n",
    "    get_datetime = udf(lambda x: datetime.fromtimestamp(int(int(x)/1000)))\r\n",
    "    get_week = udf(lambda x: calendar.day_name[x.weekday()])\r\n",
    "    get_weekday = udf(lambda x: x.isocalendar()[1])\r\n",
    "    get_hour = udf(lambda x: x.hour)\r\n",
    "    get_day = udf(lambda x : x.day)\r\n",
    "    get_year = udf(lambda x: x.year)\r\n",
    "    get_month = udf(lambda x: x.month)\r\n",
    "\r\n",
    "    # extract columns to create time table\r\n",
    "    df = df.withColumn('start_time', get_datetime(df.ts))\r\n",
    "    df = df.withColumn('hour', get_hour(df.start_time))\r\n",
    "    df = df.withColumn('day', get_day(df.start_time))\r\n",
    "    df = df.withColumn('week', get_week(df.start_time))\r\n",
    "    df = df.withColumn('month', get_month(df.start_time))\r\n",
    "    df = df.withColumn('year', get_year(df.start_time))\r\n",
    "    df = df.withColumn('weekday', get_weekday(df.start_time))\r\n",
    "\r\n",
    "  \r\n",
    "    # write time table to parquet files partitioned by year and month\r\n",
    "    print (\"    Writing time table to parquet files\")\r\n",
    "    df.select('start_time', 'hour', 'day', 'week', 'month', 'year', 'weekday') \\\r\n",
    "        .write.mode(\"overwrite\") \\\r\n",
    "        .parquet(output_data + \"time\")\r\n",
    "\r\n",
    "    # read in song data to use for songplays table\r\n",
    "    song_df = spark.sql(\"SELECT DISTINCT song_id, artist_id, artist_name FROM songsData\")\r\n",
    "\r\n",
    "    # extract columns from joined song and log datasets to create songplays table\r\n",
    "    songplays_table = df.join(song_df, song_df.artist_name == df.artist, \"inner\") \\\r\n",
    "        .distinct() \\\r\n",
    "        .select(col(\"start_time\"), col(\"userId\"), col(\"level\"), col(\"sessionId\"), \\\r\n",
    "                col(\"location\"), col(\"userAgent\"), col(\"song_id\"), col(\"artist_id\")) \\\r\n",
    "        .withColumn(\"songplay_id\", monotonically_increasing_id())\r\n",
    "\r\n",
    "    # write songplays table to parquet files partitioned by year and month\r\n",
    "    print (\"    Writing songplays table to parquet files\")\r\n",
    "    songplays_table.select('*') \\\r\n",
    "      .write.mode(\"overwrite\") \\\r\n",
    "      .parquet(output_data + \"songplays\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "spark = SparkSession \\\r\n",
    "    .builder \\\r\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.4\") \\\r\n",
    "    .getOrCreate()\r\n",
    "\r\n",
    "log_data = os.path.join(\"data/log_data/*/*/*.json\")\r\n",
    "\r\n",
    "# read log data file\r\n",
    "print (\"    Reading log data file\") \r\n",
    "df = spark.read.json(log_data)\r\n",
    "dfNextSong = df.filter(df.page == \"NextSong\")\r\n",
    "dfNextSong.toPandas()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "print (\"1. Creating/Getting Spark session\")\r\n",
    "start_time = time.time()\r\n",
    "spark = create_spark_session()\r\n",
    "print(\"--- It took %s seconds ---\" % (time.time() - start_time))\r\n",
    "\r\n",
    "#input_data = \"s3a://udacity-dend/\"\r\n",
    "input_data = \"data/\"\r\n",
    "output_data = \"data/parquets/\"\r\n",
    "\r\n",
    "start_time = time.time()\r\n",
    "print (\"Starting SONG data processing\")    \r\n",
    "process_song_data(spark, input_data, output_data)\r\n",
    "print(\"--- It took %s seconds ---\" % (time.time() - start_time))\r\n",
    "\r\n",
    "start_time = time.time()\r\n",
    "print (\"Starting LOG data processing\")\r\n",
    "process_log_data(spark, input_data, output_data)\r\n",
    "print(\"--- It took %s seconds ---\" % (time.time() - start_time))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Creating/Getting Spark session\n",
      "Starting song data processing\n",
      "    Reading song data file\n",
      "    Writing songs table to parquet files\n",
      "    Writing artists table to parquet files\n",
      "Starting LOG data processing\n",
      "    Reading log data file\n",
      "    Writing users table to parquet files\n",
      "    Writing time table to parquet files\n",
      "    Writing songplays table to parquet files\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "e254ffc773357790aa04a01fb60c7c6721ec5a0c6f1763bcb2e925b3d380624c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}