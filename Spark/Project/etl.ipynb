{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "import configparser\r\n",
    "from datetime import datetime\r\n",
    "import os\r\n",
    "import calendar\r\n",
    "import time\r\n",
    "from pyspark.sql import SparkSession\r\n",
    "from pyspark.sql import functions as f\r\n",
    "from pyspark.sql import types as t"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "config = configparser.ConfigParser()\r\n",
    "config.read('dl.cfg')\r\n",
    "\r\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]= config['default']['AWS_ACCESS_KEY_ID']\r\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]= config['default']['AWS_SECRET_ACCESS_KEY']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "def create_spark_session():\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "    Description: This function  creates or get (if already exists) a Spark session \r\n",
    "    \r\n",
    "    Arguments:\r\n",
    "        None\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        spark: Spark session\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    spark = SparkSession \\\r\n",
    "        .builder \\\r\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.4\") \\\r\n",
    "        .getOrCreate()\r\n",
    "    return spark"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "def process_song_data(spark, input_data, output_data):\r\n",
    "\r\n",
    "   \"\"\"\r\n",
    "   Description: This function process songs data and write songs and artists tables to parquet files\r\n",
    "    \r\n",
    "   Arguments:\r\n",
    "      spark: Spark session\r\n",
    "      input_data: input files repository path\r\n",
    "      output_data: output files repository path\r\n",
    "   Returns:\r\n",
    "      None\r\n",
    "   \"\"\"\r\n",
    "\r\n",
    "   # get filepath to song data file\r\n",
    "   song_data = os.path.join(input_data, \"song_data/A/*/*/*.json\")\r\n",
    "\r\n",
    "   # read song data file\r\n",
    "   print (\"    Reading song data file\") \r\n",
    "   df = spark.read.json(song_data)\r\n",
    "\r\n",
    "   # write songs table to parquet files partitioned by year and artist\r\n",
    "   print (\"    Writing songs table to parquet files\")\r\n",
    "\r\n",
    "   df.drop_duplicates(subset=['song_id']).select(\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\") \\\r\n",
    "      .write.mode(\"overwrite\") \\\r\n",
    "      .partitionBy(\"year\",\"artist_id\") \\\r\n",
    "      .parquet(output_data + \"songs\")\r\n",
    "\r\n",
    "   # write artists table to parquet files\r\n",
    "   print (\"    Writing artists table to parquet files\")\r\n",
    "   \r\n",
    "   df.drop_duplicates(subset=['artist_id']).select(\"artist_id\", \"artist_name\", \"artist_location\", \"artist_latitude\", \"artist_longitude\") \\\r\n",
    "        .write.mode(\"overwrite\") \\\r\n",
    "        .parquet(output_data + \"artists\")\r\n",
    "      \r\n",
    "   df.createOrReplaceTempView(\"songsData\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "def process_log_data(spark, input_data, output_data):\r\n",
    "    \r\n",
    "    \"\"\"\r\n",
    "    Description: This function process logs data and write users, time and songplays tables to parquet files\r\n",
    "        \r\n",
    "    Arguments:\r\n",
    "        spark: Spark session\r\n",
    "        input_data: input files repository path\r\n",
    "        output_data: output files repository path\r\n",
    "    Returns:\r\n",
    "        None\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    # get filepath to log data file\r\n",
    "    log_data = os.path.join(input_data, \"log_data/*/*/*.json\")\r\n",
    "\r\n",
    "    # read log data file\r\n",
    "    print (\"    Reading log data file\") \r\n",
    "    df = spark.read.json(log_data)\r\n",
    "    \r\n",
    "    # write users table to parquet files\r\n",
    "    print (\"    Writing users table to parquet files\")\r\n",
    "\r\n",
    "    df.drop_duplicates(subset=['userId']).select(df.userId.alias(\"user_id\"), df.firstName.alias(\"first_name\") , \\\r\n",
    "        df.lastName.alias(\"last_name\"), \"gender\", \"level\") \\\r\n",
    "        .write.mode(\"overwrite\") \\\r\n",
    "        .parquet(output_data + \"users\")\r\n",
    "\r\n",
    "    # create timestamp column from original timestamp column\r\n",
    "    get_timestamp = f.udf(lambda x: str(int(int(x)/1000)))\r\n",
    "    df = df.withColumn('timestamp', get_timestamp(df.ts))\r\n",
    "\r\n",
    "    # create datetime column from original timestamp column\r\n",
    "\r\n",
    "    get_week = f.udf(lambda x: calendar.day_name[x.weekday()])\r\n",
    "    get_weekday = f.udf(lambda x: x.isocalendar()[1])\r\n",
    "    get_hour = f.udf(lambda x: x.hour)\r\n",
    "    get_day = f.udf(lambda x : x.day)\r\n",
    "    get_year = f.udf(lambda x: x.year)\r\n",
    "    get_month = f.udf(lambda x: x.month)\r\n",
    "\r\n",
    "    # extract columns to create time table\r\n",
    "    df = df.withColumn('start_time', (df['ts']/1000).cast('timestamp'))\r\n",
    "    df = df.withColumn('hour', get_hour(df.start_time))\r\n",
    "    df = df.withColumn('day', get_day(df.start_time))\r\n",
    "    df = df.withColumn('week', get_week(df.start_time))\r\n",
    "    df = df.withColumn('month', get_month(df.start_time))\r\n",
    "    df = df.withColumn('year', get_year(df.start_time))\r\n",
    "    df = df.withColumn('weekday', get_weekday(df.start_time))\r\n",
    "\r\n",
    "    # write time table to parquet files partitioned by year and month\r\n",
    "    print (\"    Writing time table to parquet files\")\r\n",
    "    df.drop_duplicates(subset=['start_time']).select('start_time', 'hour', 'day', 'week', 'month', 'year', 'weekday') \\\r\n",
    "        .write.mode(\"overwrite\") \\\r\n",
    "        .partitionBy(\"year\", \"month\") \\\r\n",
    "        .parquet(output_data + \"time\")\r\n",
    "    \r\n",
    "    # read in song data to use for songplays table\r\n",
    "    song_df = spark.sql(\"SELECT DISTINCT song_id, artist_id, artist_name FROM songsData\")\r\n",
    "\r\n",
    "    # filter by actions for song plays\r\n",
    "    dfNextSong = df.filter(df.page == \"NextSong\")\r\n",
    "\r\n",
    "    # extract columns from joined song and log datasets to create songplays table\r\n",
    "    print (\"    Writing songplays table to parquet files\")\r\n",
    "    dfNextSong.join(song_df, song_df.artist_name == df.artist, \"inner\") \\\r\n",
    "        .distinct() \\\r\n",
    "        .select(f.col(\"start_time\"), f.col(\"userId\"), f.col(\"level\"), f.col(\"sessionId\"), \\\r\n",
    "                f.col(\"location\"), f.col(\"userAgent\"), f.col(\"song_id\"), f.col(\"artist_id\"), \\\r\n",
    "                df['year'].alias('year'), df['month'].alias('month')) \\\r\n",
    "        .withColumn(\"songplay_id\", f.monotonically_increasing_id()) \\\r\n",
    "        .write.mode(\"overwrite\") \\\r\n",
    "        .partitionBy('year', 'month') \\\r\n",
    "        .parquet(output_data + \"songplays\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "print(\"BEGINNING\")\r\n",
    "print (\"1. Creating/Getting Spark session\")\r\n",
    "start_time = time.time()\r\n",
    "spark = create_spark_session()\r\n",
    "print(\"--- It took %s seconds ---\" % (time.time() - start_time))\r\n",
    "\r\n",
    "# S3 buckets\r\n",
    "# input_data = \"s3a://udacity-dend/\"\r\n",
    "# output_data = \"s3a://pg-west2-udacity/parquets/\"\r\n",
    "\r\n",
    "# Local for dev purpose\r\n",
    "input_data = \"data/\"\r\n",
    "output_data = \"data/parquets/\"\r\n",
    "\r\n",
    "start_time = time.time()\r\n",
    "print (\"2. Starting SONG data processing\")    \r\n",
    "process_song_data(spark, input_data, output_data)\r\n",
    "print(\"--- It took %s seconds ---\" % (time.time() - start_time))\r\n",
    "\r\n",
    "start_time = time.time()\r\n",
    "print (\"3. Starting LOG data processing\")\r\n",
    "process_log_data(spark, input_data, output_data)\r\n",
    "print(\"--- It took %s seconds ---\" % (time.time() - start_time))\r\n",
    "print(\"END\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BEGINNING\n",
      "1. Creating/Getting Spark session\n",
      "--- It took 0.0050008296966552734 seconds ---\n",
      "2. Starting SONG data processing\n",
      "    Reading song data file\n",
      "    Writing songs table to parquet files\n",
      "    Writing artists table to parquet files\n",
      "--- It took 8.848398685455322 seconds ---\n",
      "3. Starting LOG data processing\n",
      "    Reading log data file\n",
      "    Writing users table to parquet files\n",
      "    Writing time table to parquet files\n",
      "    Writing songplays table to parquet files\n",
      "--- It took 44.25335383415222 seconds ---\n",
      "END\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "e254ffc773357790aa04a01fb60c7c6721ec5a0c6f1763bcb2e925b3d380624c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}