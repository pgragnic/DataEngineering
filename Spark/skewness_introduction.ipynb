{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from pyspark.sql import SparkSession\r\n",
    "import pyspark.sql.functions as psf"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Create Spark session\r\n",
    "spark = SparkSession.builder.appName(\"Skewness Introduction\").getOrCreate()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#TODO get your file path from s3 for parking_violation.csv\r\n",
    "input_path = \"c:/temp/parking_violation.csv\"\r\n",
    "df = spark.read.format(\"csv\").option(\"header\", True).load(input_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# investigate what columns you have\r\n",
    "col_list = df.columns\r\n",
    "print(col_list)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['_c0', 'Summons_Number', 'Plate_ID', 'Registration_State', 'Plate_Type', 'Issue_Date', 'Violation_Code', 'Vehicle_Body_Type', 'Vehicle_Make', 'Issuing_Agency', 'Street_Code1', 'Street_Code2', 'Street_Code3', 'Vehicle_Expiration_Date', 'Violation_Location', 'Violation_Precinct', 'Issuer_Precinct', 'Issuer_Code', 'Issuer_Command', 'Issuer_Squad', 'Violation_Time', 'Time_First_Observed', 'Violation_County', 'Violation_In_Front_Of_Or_Opposite', 'House_Number', 'Street_Name', 'Intersecting_Street', 'Date_First_Observed', 'Law_Section', 'Sub_Division', 'Violation_Legal_Code', 'Days_Parking_In_Effect____', 'From_Hours_In_Effect', 'To_Hours_In_Effect', 'Vehicle_Color', 'Unregistered_Vehicle?', 'Vehicle_Year', 'Meter_Number', 'Feet_From_Curb', 'Violation_Post_Code', 'Violation_Description', 'No_Standing_or_Stopping_Violation', 'Hydrant_Violation', 'Double_Parking_Violation', 'Latitude', 'Longitude', 'Community_Board', 'Community_Council_', 'Census_Tract', 'BIN', 'BBL', 'NTA', 'year', 'month']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# TODO groupby month and year to get count\r\n",
    "year_df = df.groupby(\"year\")\r\n",
    "month_df = df.groupby(\"month\")\r\n",
    "\r\n",
    "year_df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x142d25ee460>"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "    # TODO write file partition by year, and study the executor in the spark UI\r\n",
    "    # TODO use repartition function"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "e254ffc773357790aa04a01fb60c7c6721ec5a0c6f1763bcb2e925b3d380624c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}