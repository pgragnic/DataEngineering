{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from pyspark.sql import SparkSession\r\n",
    "from pyspark.sql.functions import udf\r\n",
    "from pyspark.sql.types import StringType\r\n",
    "from pyspark.sql.types import IntegerType\r\n",
    "from pyspark.sql.functions import desc\r\n",
    "from pyspark.sql.functions import asc\r\n",
    "from pyspark.sql.functions import sum as Fsum\r\n",
    "from pyspark import SparkConf, SparkContext\r\n",
    "\r\n",
    "import datetime\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import boto3\r\n",
    "import configparser\r\n",
    "\r\n",
    "import pyspark\r\n",
    "import os\r\n",
    "\r\n",
    "#os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.1 pyspark-shell --master local[2] pyspark-shell --conf spark.hadoop.fs.s3a.endpoint=s3.us-west-2.amazonaws.com'\r\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.3 pyspark-shell --master local[2] pyspark-shell --conf spark.hadoop.fs.s3a.endpoint=s3.eu-west-3.amazonaws.com'\r\n",
    "#org.apache.hadoop:hadoop-aws:2.7.4 before\r\n",
    "%matplotlib inline\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "import findspark\r\n",
    "findspark.init()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "config = configparser.ConfigParser()\r\n",
    "config.read_file(open('dwh.cfg'))\r\n",
    "\r\n",
    "KEY                    = config.get('AWS','KEY')\r\n",
    "SECRET                 = config.get('AWS','SECRET')\r\n",
    "\r\n",
    "s3 = boto3.resource('s3',\r\n",
    "                       region_name=\"eu-west-3\",\r\n",
    "                       aws_access_key_id=KEY,\r\n",
    "                       aws_secret_access_key=SECRET\r\n",
    "                   )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#spark configuration\r\n",
    "#conf = SparkConf().set(\"spark.executor.extraJavaOptions\",\"-Dcom.amazonaws.services.s3.enableV4=true\"). \\\r\n",
    "# set(\"spark.driver.extraJavaOptions\",\"-Dcom.amazonaws.services.s3.enableV4=true\"). \\\r\n",
    "# setAppName(\"data_wrangling\").setMaster(\"local[*]\")\r\n",
    "\r\n",
    "#sc=SparkContext(conf=conf)\r\n",
    "sc = pyspark.SparkContext(appName=\"data wrangling 2\")\r\n",
    "sc.setSystemProperty(\"com.amazonaws.services.s3.enableV4\", \"true\")\r\n",
    "\r\n",
    "accessKeyId=KEY\r\n",
    "secretAccessKey=SECRET\r\n",
    "\r\n",
    "hadoopConf = sc._jsc.hadoopConfiguration()\r\n",
    "hadoopConf.set(\"fs.s3a.access.key\", accessKeyId)\r\n",
    "hadoopConf.set(\"fs.s3a.secret.key\", secretAccessKey)\r\n",
    "hadoopConf.set(\"fs.s3a.endpoint\", \"s3.eu-west-3.amazonaws.com\")\r\n",
    "hadoopConf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\r\n",
    "\r\n",
    "spark=SparkSession(sc)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "path = \"s3a://pg-udacity2/songplays-small.csv\"\r\n",
    "#obj = s3.getObject(\"pg-udacity2\",\"songplays.csv\")\r\n",
    "#print(obj.getObjectContent())\r\n",
    "\r\n",
    "#bucket = s3.Bucket('pg-udacity2')\r\n",
    "# Iterates through all the objects, doing the pagination for you. Each obj\r\n",
    "# is an ObjectSummary, so it doesn't contain the body. You'll need to call\r\n",
    "# get to get the whole body.\r\n",
    "#for obj in bucket.objects.all():\r\n",
    "#    key = obj.key\r\n",
    "#    body = obj.get()['Body'].read()\r\n",
    "#    print (body)\r\n",
    "\r\n",
    "#user_log=spark.read.json(path)\r\n",
    "user_log=spark.read.csv(path,header=False,inferSchema=True)\r\n",
    "user_log.show(10)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------------+\n",
      "|            _c0|\n",
      "+---------------+\n",
      "|    Deep Dreams|\n",
      "|Data House Rock|\n",
      "|    Deep Dreams|\n",
      "|Data House Rock|\n",
      "|Broken Networks|\n",
      "|Data House Rock|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "e254ffc773357790aa04a01fb60c7c6721ec5a0c6f1763bcb2e925b3d380624c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}